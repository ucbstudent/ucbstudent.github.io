---
layout: post
title:  "Spark"
date:   2015-9-8 15:00
permalink: spark
---
Before spark, batch processing systems had implicitly assumed that data 
processing computation graphs were acyclic -- the result of a particular
step need not feed back on itself.  Of course, one could implement cyclic
data processing jobs by executing multiple map-reduce jobs in succession,
but this is neither natural nor efficient.  Spark recognises the need to
support these sorts of computations and provides convincing motivating
examples.  I consider it a real problem.

I think the most important idea here is getting the data structures right.
Dryad, MapReduce, Naiad, etc, are mostly about structuring the computation --
what happens in what order -- Spark is about structuring the *data*.  Once
that's right, doing the actually computation boils down to some cute syntactic
sugar on top of Scala.  By getting the data-structures right, Spark can
transparently, distribute computation, use lineage information to reconstruct
lost data, and allow the programmer to hint at what data should stay in memory
for cyclic computations.

Spark Seems to be enabled by some slightly different assumptions from the
previous work.  Specifically, the cost of memory.  Back when MapReduce and
Dryad were written, the idea of holding large working sets in memory was so
cost prohibitive as to be unimaginable.  As memory becomes cheaper, figuring
out what to store, how to store it, and where to store it becomes a much bigger
issue as memory vs disk access is not uniform.

I think the paper will pretty clearly be influential for some time to come, but
not necessarily it's specific research contributions.  The example code in the
paper seems so easy to understand, the learning curve for the average
developer/data scientist is not intimidating.  This fact coupled with it's
performance advantages has driven adoption and, therefore, impact.
